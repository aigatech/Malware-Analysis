# Malware-Analysis

## Environment Setup

Install poetry
`pip install poetry`

Install dependencies
`poetry install`

Enter Poetry Shell
`poetry shell`

## Running the CFG Creator

To generate Control Flow Graphs (CFGs) from binary files, use the `cfg_creator.py` script. This script analyzes binary files and creates CFGs, which can be visualized or saved in different formats.

### Usage

Run the script using the following command:
`python cfg_creator.py --data_dir <path_to_binary_files> --vis_mode <visualization_mode>`

Args:
- `--data_dir`: Path to the directory containing the binary files in the parent directory. (str, default='data')
- `--vis_mode`: Visualization mode. 0 = visualize in window, 1 = save as HTML docs, 2 = save graphs w/o visualizing as edgelists and `csv` for node values. (int, default=2)

Output will be stored dependent on the vis_mode:
- vis_mode=0: no output
- vis_mode=1: HTML files in `cfg_dataset/out_html`
- vis_mode=2: CSV files in `cfg_dataset/out_edgelists`

## Methodology
Using Static Analysis (deconstruction of binaries without execution) to extract Control Flow Graphs from a binary.

Leverage Graph Neural Networks trained on these CFGs to classify an arbitrary binary as malicious or benign.
We aim to primarily utilize a dataset of 200k+ Windows PE binaries [linked here](https://practicalsecurityanalytics.com/pe-malware-machine-learning-dataset/)

## Goal
Produce a pipeline capable of performing deconstruction + inference **very fast**. 

Feature based models (i.e. XGBoost -> tree model, Yara Rules -> condition matching) can run in <1s and NLP tools (i.e. Kilogram paper -> n-gram analysis) can also run fairly fast. 

Our hypothesis is that GNNs can capture more complex characteristics of malicious binaries via their CFGs and by training a large model and compressing it to a smaller downstream one, we can match the accuracy of feature based approaches with a fairly close inference time as well.

## Compression Techniques

TBD - do more research here. Added potential ones but requires more insight
- Distillation (teaching a smaller downstream model to learn the behavior of the large model; famous from DistilBERT)
- Quantization (possibly quantization aware training to facilitate this approach)
- Pruning (self explanatory; remove weights determined as irrelevant by some arbitrary technique)
- Theoretical Optimization based on BERT-of-Theseus paper
  - Paper is centered around replacing large BERT modules with small modules while training to get small modules to mimic behavior of large ones in network
  - Depends on GNN architecture but could this be applied here? Can we have an optimization method where modules with 50%, 75% less params than large-GNN modules are randomly inserted in some trained network and trained to mimic the role of the large modules (similar to distillation but incorporated in the network)?
